{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4f31d5",
   "metadata": {},
   "source": [
    "# keep only 105031 rows from the dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208377e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gender ---\n",
      "Gender\n",
      "Male      52517\n",
      "Female    52514\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Country ---\n",
      "Country\n",
      "United States             65310\n",
      "United Kingdom            16682\n",
      "Canada                     7159\n",
      "Australia                  2675\n",
      "Ireland                    1891\n",
      "Netherlands                1626\n",
      "Sweden                     1294\n",
      "Germany                    1023\n",
      "India                       945\n",
      "New Zealand                 776\n",
      "South Africa                775\n",
      "Belgium                     520\n",
      "Poland                      519\n",
      "France                      513\n",
      "Brazil                      513\n",
      "Switzerland                 341\n",
      "Israel                      340\n",
      "Italy                       340\n",
      "Russia                      171\n",
      "Greece                      170\n",
      "Singapore                   170\n",
      "Denmark                     170\n",
      "Costa Rica                   86\n",
      "Portugal                     86\n",
      "Finland                      86\n",
      "Czech Republic               85\n",
      "Georgia                      85\n",
      "Colombia                     85\n",
      "Moldova                      85\n",
      "Thailand                     85\n",
      "Mexico                       85\n",
      "Croatia                      85\n",
      "Nigeria                      85\n",
      "Bosnia and Herzegovina       85\n",
      "Philippines                  85\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Occupation ---\n",
      "Occupation\n",
      "Housewife    23301\n",
      "Others       22333\n",
      "Corporate    21869\n",
      "Student      21819\n",
      "Business     15709\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- self_employed ---\n",
      "self_employed\n",
      "0.0    94115\n",
      "1.0     8757\n",
      "NaN     2159\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- family_history ---\n",
      "family_history\n",
      "0    54998\n",
      "1    50033\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- treatment ---\n",
      "treatment\n",
      "1    60784\n",
      "0    44247\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Days_Indoors ---\n",
      "Days_Indoors\n",
      "31-60 days            24420\n",
      "More than 2 months    21476\n",
      "Go out Every day      21466\n",
      "15-30 days            19389\n",
      "1-14 days             18280\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Growing_Stress ---\n",
      "Growing_Stress\n",
      "1.0    42458\n",
      "0.5    32577\n",
      "0.0    29996\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Changes_Habits ---\n",
      "Changes_Habits\n",
      "1.0    38798\n",
      "0.5    33293\n",
      "0.0    32940\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Mental_Health_History ---\n",
      "Mental_Health_History\n",
      "0.0    35138\n",
      "0.5    35108\n",
      "1.0    34785\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Mood_Swings ---\n",
      "Mood_Swings\n",
      "0.5    37346\n",
      "0.0    36489\n",
      "1.0    31196\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Coping_Struggles ---\n",
      "Coping_Struggles\n",
      "0    55313\n",
      "1    49718\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Work_Interest ---\n",
      "Work_Interest\n",
      "0.5    38455\n",
      "0.0    34503\n",
      "1.0    32073\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Social_Weakness ---\n",
      "Social_Weakness\n",
      "0.0    36479\n",
      "1.0    34412\n",
      "0.5    34140\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- mental_health_interview ---\n",
      "mental_health_interview\n",
      "0.0    87775\n",
      "0.5    15029\n",
      "1.0     2227\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- care_options ---\n",
      "care_options\n",
      "0.0    37566\n",
      "1.0    37219\n",
      "0.5    30246\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Mental Health DataSet.xlsx\",nrows=105031)\n",
    "for col in df.columns:\n",
    "    print(f\"--- {col} ---\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e517df0",
   "metadata": {},
   "source": [
    "# remove NaN values and Encoding categorical features: Convert Gender, Country, Occupation, and Days_Indoors into numeric representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65e562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "United States     63581\n",
      "United Kingdom    16510\n",
      "Other              7371\n",
      "Canada             6901\n",
      "Australia          2675\n",
      "Ireland            1891\n",
      "Netherlands        1626\n",
      "Sweden             1294\n",
      "Germany            1023\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove rows with any NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Map Days_Indoors to an ordinal integer\n",
    "order = [\n",
    "    \"Go out Every day\",  # least indoor\n",
    "    \"1-14 days\",\n",
    "    \"15-30 days\",\n",
    "    \"31-60 days\",\n",
    "    \"More than 2 months\" # most indoor\n",
    "]\n",
    "mapping = {cat: i for i, cat in enumerate(order)}\n",
    "df['Days_Indoors'] = df['Days_Indoors'].map(mapping)\n",
    "\n",
    "# 3. replace rare countries with 'Other'\n",
    "threshold = 1000  \n",
    "country_counts = df['Country'].value_counts()\n",
    "rare_countries = country_counts[country_counts < threshold].index\n",
    "\n",
    "df['Country'] = df['Country'].apply(lambda x: 'Other' if x in rare_countries else x)\n",
    "print(df['Country'].value_counts())\n",
    "\n",
    "# 4. One-hot encode the remaining true categoricals\n",
    "nominals = [\n",
    "    'Gender',\n",
    "    'Country',\n",
    "    'Occupation',\n",
    "]\n",
    "\n",
    "# Only run get_dummies if ALL nominal columns are still in the DataFrame\n",
    "if set(nominals).issubset(df.columns):\n",
    "    df = pd.get_dummies(df, columns=nominals, drop_first=False, dtype=int)\n",
    "\n",
    "df.to_excel(\"cleaned_dataset.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd20e2",
   "metadata": {},
   "source": [
    "# Now Train/test split to prepare for model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5594009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (82297, 28)\n",
      "X_test shape:  (20575, 28)\n",
      "y_train shape: (82297,)\n",
      "y_test shape:  (20575,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.drop('treatment', axis=1)\n",
    "y = df['treatment']\n",
    "\n",
    "# First split: 80% temp + 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3e307",
   "metadata": {},
   "source": [
    "# Decision Tree code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc64b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class DecisionTreeClassifierScratch:\n",
    "    \"\"\"\n",
    "    A simple Decision Tree classifier implemented from scratch.\n",
    "    Supports 'entropy' or 'gini' as split criteria.\n",
    "    \"\"\"\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def __init__(self, max_depth=5, min_samples_split=10, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
    "\n",
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return 1 - np.sum(probs**2)\n",
    "\n",
    "    # Measures how much better the split separates the classes\n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        if self.criterion == 'gini':\n",
    "            loss = self._gini\n",
    "        else:\n",
    "            loss = self._entropy\n",
    "        parent_loss = loss(y)\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        child_loss = (n_left/n)*loss(y_left) + (n_right/n)*loss(y_right)\n",
    "        return parent_loss - child_loss # highter is better\n",
    "\n",
    "    # find the best feature and threshold to split the dataset\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_feat, best_thresh = None, None\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # get all the unique values\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for t in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= t\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feat, best_thresh = gain, feature_idx, t\n",
    "\n",
    "        return best_feat, best_thresh\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return classes[np.argmax(counts)]\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # Stopping conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) \\\n",
    "           or len(np.unique(y)) == 1 \\\n",
    "           or len(y) < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeClassifierScratch.Node(value=leaf_value)\n",
    "        \n",
    "        feat, thresh = self._best_split(X, y)\n",
    "        if feat is None:\n",
    "            return DecisionTreeClassifierScratch.Node(value=self._most_common_label(y))\n",
    "\n",
    "        left_mask = X[:, feat] <= thresh\n",
    "        left = self._build_tree(X[left_mask], y[left_mask], depth+1)\n",
    "        right = self._build_tree(X[~left_mask], y[~left_mask], depth+1)\n",
    "        return DecisionTreeClassifierScratch.Node(feature=feat, threshold=thresh, left=left, right=right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree using training data.\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        y: array-like of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.tree = self._build_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91400542",
   "metadata": {},
   "source": [
    "# Metric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da1f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                         Predicted 0  Predicted 1\n",
      "Actual 0 (No Treatment)         5829         2917\n",
      "Actual 1 (Treatment)            1836         9993\n",
      "\n",
      "Accuracy:  0.7690\n",
      "Precision: 0.7741\n",
      "Recall:    0.8448\n",
      "F1-Score:  0.8079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# --- Train scratch Decision Tree ---\n",
    "tree = DecisionTreeClassifierScratch(max_depth=15, min_samples_split=10, criterion='entropy')\n",
    "tree.fit(X_train.values, y_train.values)\n",
    "\n",
    "# --- Predict on validation set (or use X_test for final) ---\n",
    "y_pred = tree.predict(X_test.values)  # For validation\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)  # or y_test, y_pred for test set\n",
    "\n",
    "# --- Compute metrics ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# --- Output results ---\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=['Actual 0 (No Treatment)', 'Actual 1 (Treatment)'], columns=['Predicted 0', 'Predicted 1']))\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97141ef3",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9918e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class DecisionTreeClassifierRF(DecisionTreeClassifierScratch):\n",
    "    def __init__(self, max_depth=15, min_samples_split=20, criterion='entropy',\n",
    "                 max_features=5, random_state=42):\n",
    "        super().__init__(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion)\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # randomly select a subset of features\n",
    "        if self.max_features and self.max_features < n_features:\n",
    "            features = self.rng.choice(n_features, self.max_features, replace=False)\n",
    "        else:\n",
    "            features = np.arange(n_features)\n",
    "\n",
    "        best_gain = 0\n",
    "        best_feat, best_thresh = None, None\n",
    "        for feature_idx in features:\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for t in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= t\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feat, best_thresh = gain, feature_idx, t\n",
    "        return best_feat, best_thresh\n",
    "\n",
    "class RandomForestClassifierScratch:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=20,\n",
    "                 criterion='entropy', max_features='sqrt', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _get_max_features(self, n_features):\n",
    "        if isinstance(self.max_features, int):\n",
    "            return self.max_features\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        if self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        return n_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = self.rng.choice(n_samples, n_samples//2, replace=True)\n",
    "            X_sample, y_sample = X[indices], y[indices]\n",
    "\n",
    "            max_feats = self._get_max_features(n_features)\n",
    "            tree = DecisionTreeClassifierRF(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                criterion=self.criterion,\n",
    "                max_features=max_feats,\n",
    "                random_state=(self.random_state + i) if self.random_state is not None else None\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        # Collect predictions from each tree\n",
    "        all_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Majority vote\n",
    "        y_pred = []\n",
    "        for preds in all_preds.T:\n",
    "            vote = Counter(preds).most_common(1)[0][0]\n",
    "            y_pred.append(vote)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49046f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Confusion Matrix:\n",
      "                         Predicted 0  Predicted 1\n",
      "Actual 0 (No Treatment)         5737         3009\n",
      "Actual 1 (Treatment)            1548        10281\n",
      "\n",
      "Accuracy:    0.7785\n",
      "Precision:   0.7736\n",
      "Recall:      0.8691\n",
      "F1-Score:    0.8186\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Train scratch Random Forest ---\n",
    "rf = RandomForestClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    criterion='entropy',\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "# --- Predict on validation set (or test set for final eval) ---\n",
    "y_pred_rf = rf.predict(X_test.values)  # For validation\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)  # or y_test, y_pred_rf for test set\n",
    "\n",
    "# --- Compute metrics ---\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "# --- Output results ---\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(pd.DataFrame(\n",
    "    cm_rf,\n",
    "    index=['Actual 0 (No Treatment)', 'Actual 1 (Treatment)'],\n",
    "    columns=['Predicted 0', 'Predicted 1']\n",
    "))\n",
    "print(f\"\\nAccuracy:    {accuracy_rf:.4f}\")\n",
    "print(f\"Precision:   {precision_rf:.4f}\")\n",
    "print(f\"Recall:      {recall_rf:.4f}\")\n",
    "print(f\"F1-Score:    {f1_rf:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff0800",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "758d7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Decision Tree Node ---\n",
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.polarity = 1\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.prediction = None\n",
    "        self.is_leaf = False\n",
    "\n",
    "# --- Decision Tree as a weak learner ---\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y, sample_weights):\n",
    "        X, y, w = np.array(X), np.array(y), np.array(sample_weights)\n",
    "        self.root = self._build_tree(X, y, w, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build_tree(self, X, y, w, depth):\n",
    "        node = TreeNode()\n",
    "        \n",
    "        # Base cases: max depth reached or pure node\n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1 or len(y) <= 1:\n",
    "            node.is_leaf = True\n",
    "            # Weighted majority vote\n",
    "            pos_weight = np.sum(w[y == 1])\n",
    "            neg_weight = np.sum(w[y == -1])\n",
    "            node.prediction = 1 if pos_weight > neg_weight else -1\n",
    "            return node\n",
    "\n",
    "        # Find best split\n",
    "        best_feature, best_threshold, best_polarity, min_error = self._find_best_split(X, y, w)\n",
    "        \n",
    "        if best_feature is None:  # No valid split found\n",
    "            node.is_leaf = True\n",
    "            pos_weight = np.sum(w[y == 1])\n",
    "            neg_weight = np.sum(w[y == -1])\n",
    "            node.prediction = 1 if pos_weight > neg_weight else -1\n",
    "            return node\n",
    "\n",
    "        node.feature = best_feature\n",
    "        node.threshold = best_threshold\n",
    "        node.polarity = best_polarity\n",
    "\n",
    "        # Split data\n",
    "        if best_polarity == 1:\n",
    "            left_mask = X[:, best_feature] < best_threshold\n",
    "            right_mask = X[:, best_feature] >= best_threshold\n",
    "        else:\n",
    "            left_mask = X[:, best_feature] > best_threshold\n",
    "            right_mask = X[:, best_feature] <= best_threshold\n",
    "\n",
    "        # Recursively build subtrees\n",
    "        if np.any(left_mask):\n",
    "            node.left = self._build_tree(X[left_mask], y[left_mask], w[left_mask], depth + 1)\n",
    "        else:\n",
    "            # Create leaf node\n",
    "            node.left = TreeNode()\n",
    "            node.left.is_leaf = True\n",
    "            node.left.prediction = 1 if np.sum(w[y == 1]) > np.sum(w[y == -1]) else -1\n",
    "\n",
    "        if np.any(right_mask):\n",
    "            node.right = self._build_tree(X[right_mask], y[right_mask], w[right_mask], depth + 1)\n",
    "        else:\n",
    "            # Create leaf node\n",
    "            node.right = TreeNode()\n",
    "            node.right.is_leaf = True\n",
    "            node.right.prediction = 1 if np.sum(w[y == 1]) > np.sum(w[y == -1]) else -1\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _find_best_split(self, X, y, w):\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_polarity = None\n",
    "\n",
    "        for feature_i in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_i])\n",
    "            for t in thresholds:\n",
    "                for polarity in [1, -1]:\n",
    "                    # Make predictions based on split\n",
    "                    preds = np.ones(n_samples)\n",
    "                    if polarity == 1:\n",
    "                        preds[X[:, feature_i] < t] = -1\n",
    "                    else:\n",
    "                        preds[X[:, feature_i] > t] = -1\n",
    "\n",
    "                    error = np.sum(w[preds != y])\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_polarity = polarity\n",
    "                        best_threshold = t\n",
    "                        best_feature = feature_i\n",
    "\n",
    "        return best_feature, best_threshold, best_polarity, min_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return np.array([self._predict_sample(self.root, sample) for sample in X])\n",
    "\n",
    "    def _predict_sample(self, node, sample):\n",
    "        if node.is_leaf:\n",
    "            return node.prediction\n",
    "\n",
    "        if node.polarity == 1:\n",
    "            if sample[node.feature] < node.threshold:\n",
    "                return self._predict_sample(node.left, sample)\n",
    "            else:\n",
    "                return self._predict_sample(node.right, sample)\n",
    "        else:\n",
    "            if sample[node.feature] > node.threshold:\n",
    "                return self._predict_sample(node.left, sample)\n",
    "            else:\n",
    "                return self._predict_sample(node.right, sample)\n",
    "\n",
    "# --- AdaBoost with Decision Trees ---\n",
    "class AdaBoostTrees:\n",
    "    def __init__(self, n_estimators=100, max_depth=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learners = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        # convert y to {-1, +1}\n",
    "        y_signed = np.where(y == 1, 1, -1)\n",
    "        n_samples = len(y_signed)\n",
    "        # initialize sample weights\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTree(max_depth=self.max_depth).fit(X, y_signed, w)\n",
    "            preds = tree.predict(X)\n",
    "\n",
    "            # weighted error\n",
    "            err = np.clip(np.sum(w[preds != y_signed]), 1e-10, 1-1e-10)\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "            # update weights\n",
    "            w *= np.exp(-alpha * y_signed * preds)\n",
    "            w /= w.sum()\n",
    "\n",
    "            self.learners.append(tree)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        # weighted sum of tree predictions\n",
    "        learner_preds = np.array([alpha * learner.predict(X)\n",
    "                                   for learner, alpha in zip(self.learners, self.alphas)])\n",
    "        y_signed_pred = np.sign(np.sum(learner_preds, axis=0))\n",
    "        # map back to {0,1}\n",
    "        return np.where(y_signed_pred == 1, 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36612480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Confusion Matrix:\n",
      "                         Predicted 0  Predicted 1\n",
      "Actual 0 (No Treatment)         5889         2857\n",
      "Actual 1 (Treatment)            1447        10382\n",
      "\n",
      "Accuracy:  0.7908\n",
      "Precision: 0.7842\n",
      "Recall:    0.8777\n",
      "F1-Score:  0.8283\n"
     ]
    }
   ],
   "source": [
    "# --- Train AdaBoost with weak Trees ---\n",
    "ab = AdaBoostTrees(n_estimators=100, max_depth=5)\n",
    "ab.fit(X_train.values, y_train.values) \n",
    "\n",
    "y_pred_ab = ab.predict(X_test.values) \n",
    "\n",
    "# --- Evaluate ---\n",
    "cm_ab = confusion_matrix(y_test, y_pred_ab) \n",
    "\n",
    "print(\"AdaBoost Confusion Matrix:\")\n",
    "print(pd.DataFrame(\n",
    "    cm_ab,\n",
    "    index=['Actual 0 (No Treatment)', 'Actual 1 (Treatment)'],\n",
    "    columns=['Predicted 0', 'Predicted 1']\n",
    "))\n",
    "print(f\"\\nAccuracy:  {accuracy_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_ab):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
